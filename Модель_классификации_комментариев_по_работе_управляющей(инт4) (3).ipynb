{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxzYRIqr4qEv"
      },
      "source": [
        "# Загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5wMaXGg4I_-"
      },
      "source": [
        "Импорт и установка библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw9-Ruybq0MH",
        "outputId": "5a549237-eeaa-45e9-f51a-50ad161064af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.9\n",
            "Collecting ru-core-news-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.8.0/ru_core_news_md-3.8.0-py3-none-any.whl (41.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-md==3.8.0)\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-md\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ],
      "source": [
        "# Установка необходимых библиотек\n",
        "!pip install spacy nltk scikit-learn transformers datasets iterative-stratification wordcloud\n",
        "!python -m spacy download ru_core_news_md\n",
        "!pip install evaluate\n",
        "# Импорт библиотек\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import json\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import Dataset\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywi49FhK4fDP"
      },
      "source": [
        "# Анализ и предобработка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm7Nu05i4VmF"
      },
      "source": [
        "### просмотр статистической информации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "H9oKd4oy27oc",
        "outputId": "e578f435-82ea-4849-8502-27d7f452ea15"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/321.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-00524b048024>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# df2 = pd.read_csv(\"/content/project-20-at-2025-05-15-05-35-2831ec76.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df = pd.concat([df1, df2], ignore_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/321.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# === ШАГ 2: Разметка sentiment ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m known_categories = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/321.csv'"
          ]
        }
      ],
      "source": [
        "# # === ШАГ 1: Объединяем два исходных файла ===\n",
        "# df1 = pd.read_csv(\"/content/project-13-at-2025-05-11-01-19-e254a3e5.csv\")\n",
        "# df2 = pd.read_csv(\"/content/project-20-at-2025-05-15-05-35-2831ec76.csv\")\n",
        "# df = pd.concat([df1, df2], ignore_index=True)\n",
        "df = pd.read_csv(\"/content/321.csv\")\n",
        "# === ШАГ 2: Разметка sentiment ===\n",
        "known_categories = [\n",
        "    \"Вопрос решен\",\n",
        "    \"Нравится качество выполнения заявки\",\n",
        "    \"Нравится качество работы сотрудников\",\n",
        "    \"Нравится скорость отработки заявок\",\n",
        "    \"Понравилось выполнение заявки\"\n",
        "]\n",
        "\n",
        "# Добавляем колонки (в том числе \"Другое\")\n",
        "for col in known_categories + [\"Другое\"]:\n",
        "    if col not in df.columns:\n",
        "        df[col] = 0\n",
        "\n",
        "df[\"sentiment\"] = df[\"sentiment\"].fillna(\"\")\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    try:\n",
        "        sentiment_data = json.loads(row[\"sentiment\"])\n",
        "        choices = sentiment_data.get(\"choices\", [])\n",
        "\n",
        "        for col in known_categories + [\"Другое\"]:\n",
        "            df.at[i, col] = 0  # обнуляем\n",
        "\n",
        "        for choice in choices:\n",
        "            if choice in known_categories:\n",
        "                df.at[i, choice] = 1\n",
        "            else:\n",
        "                df.at[i, \"Другое\"] = 1\n",
        "    except Exception:\n",
        "        for col in known_categories + [\"Другое\"]:\n",
        "            df.at[i, col] = 0\n",
        "\n",
        "# Сохраняем объединённый с разметкой файл\n",
        "df.to_csv(\"/content/merged_with_sentiment_flags.csv\", index=False)\n",
        "\n",
        "# === ШАГ 3: Сравнение по ID с разметкой ===\n",
        "df_base = pd.read_csv(\"/content/разметка комментариев 2.csv\")\n",
        "df_other = pd.read_csv(\"/content/merged_with_sentiment_flags.csv\")\n",
        "\n",
        "if 'id' in df_base.columns and 'id' in df_other.columns:\n",
        "    df_matched = df_other[df_other['id'].isin(df_base['id'])]\n",
        "\n",
        "    # Сбрасываем индекс, чтобы он начинался с 1 (а не с 0)\n",
        "    df_matched = df_matched.reset_index(drop=True)\n",
        "    df_matched.index = df_matched.index + 1  # Делаем индексацию с 1\n",
        "\n",
        "    df_matched.to_csv(\"/content/коментарии.csv\", index=True)  # index=True сохраняет новый индекс\n",
        "    print(\"Файл коментарии.csv сохранён успешно.\")\n",
        "else:\n",
        "    print(\"В одном из файлов отсутствует колонка 'id'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Информация о DataFrame (с дубликатами) ===\")\n",
        "print(f\"Общее количество строк: {len(df_matched)}\")\n",
        "print(f\"Количество уникальных строк: {len(df_matched.drop_duplicates())}\")\n",
        "print(f\"Количество полных дубликатов: {len(df_matched) - len(df_matched.drop_duplicates())}\")"
      ],
      "metadata": {
        "id": "Wk5jX2075M_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtiKz-6lreJW"
      },
      "outputs": [],
      "source": [
        "print(\"\\n \\n ===Типы данных и пропуски: ===\")\n",
        "print(df_matched.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyH4NmGwqz-9"
      },
      "outputs": [],
      "source": [
        "df_matched.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkrdiUFmqz40"
      },
      "outputs": [],
      "source": [
        "df_matched.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C3qF68qXa90"
      },
      "outputs": [],
      "source": [
        "df_matched = df_matched.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyPpQ5d-ECWg"
      },
      "outputs": [],
      "source": [
        "df_matched.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94afbtxuECJh"
      },
      "outputs": [],
      "source": [
        "df_matched.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eAhNszO6RSL"
      },
      "outputs": [],
      "source": [
        "#Удаление колонок не нужных для обучения модели и не применяющиеся в анализе\n",
        "df_clean =df_matched.drop(columns=['annotation_id', 'annotator', 'id','updated_at', 'lead_time','sentiment', 'created_at'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDRwTHZhuiSu"
      },
      "outputs": [],
      "source": [
        "df_clean.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00FMSImgiQ1R"
      },
      "outputs": [],
      "source": [
        "df_clean.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKyW8cId5HMD"
      },
      "source": [
        "### Визуализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5auT0fHRlHj"
      },
      "outputs": [],
      "source": [
        "# Маска из изображения здания\n",
        "mask = np.array(Image.open(\"/content/building (1).png\"))\n",
        "\n",
        "# Объединяем все комментарии в одну строку\n",
        "text_all = \" \".join(df_clean['comment'].dropna())\n",
        "\n",
        "# Создание облака слов без стоп-слов\n",
        "wordcloud = WordCloud(\n",
        "    width=2000,\n",
        "    height=1800,\n",
        "    background_color='white',\n",
        "    mask=mask,\n",
        "    contour_color='black',\n",
        "    contour_width=3,\n",
        "    max_words=1500,\n",
        "    colormap='plasma',\n",
        "    prefer_horizontal=0.95,\n",
        "    max_font_size=200,\n",
        "    scale=5,  # увеличивает детализацию!\n",
        "    random_state=42\n",
        ").generate(text_all)\n",
        "\n",
        "# Отображение\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На этом графике можно уведить что больше всего люди пишут отзывы используя такие слова как спасибо,быстро,оперативно и т.д"
      ],
      "metadata": {
        "id": "D6BaLKJsBUsA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EvkOwU8nYKb"
      },
      "outputs": [],
      "source": [
        "# Явно указываем столбцы для исключения\n",
        "exclude_cols = ['comment', 'rating']\n",
        "category_cols = [col for col in df_clean.columns if col not in exclude_cols]\n",
        "\n",
        "# Строим график\n",
        "plt.figure(figsize=(10, 6))\n",
        "df_clean[category_cols].sum().sort_values().plot(\n",
        "    kind='barh',\n",
        "    color='red',\n",
        "    edgecolor='darkblue',  # Добавляем границы для лучшей читаемости\n",
        "    alpha=0.7  # Полупрозрачность\n",
        ")\n",
        "\n",
        "# Улучшаем оформление\n",
        "plt.title(\"Количество положительных откликов по категориям\", pad=20, fontsize=14)\n",
        "plt.xlabel(\"Количество\", labelpad=10)\n",
        "plt.ylabel(\"Категория\", labelpad=10)\n",
        "plt.grid(True, axis='x', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Добавляем значения на столбцы\n",
        "for i, v in enumerate(df_clean[category_cols].sum().sort_values()):\n",
        "    plt.text(v + 0.5, i, str(v), color='black', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На этом графике можно заменить очень сильный дисбаланс классов"
      ],
      "metadata": {
        "id": "q2ZMW6FKB6qe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giCBWS_dRkuq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Подсчёт и нормализация\n",
        "rating_percent = df_clean['rating'].value_counts(normalize=True).sort_index() * 100\n",
        "\n",
        "# Настройка стиля\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Создаем цветовую палитру\n",
        "colors = sns.color_palette(\"viridis\", len(rating_percent))\n",
        "\n",
        "# Построение графика\n",
        "bars = plt.bar(rating_percent.index.astype(str), rating_percent.values, color=colors)\n",
        "\n",
        "# Добавляем данные на каждый столбец\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{height:.1f}%',\n",
        "             ha='center', va='bottom',\n",
        "             fontsize=10)\n",
        "\n",
        "# Настройка оформления\n",
        "plt.title(\"Распределение оценок (rating)\\n\", fontsize=14, fontweight='bold')\n",
        "plt.ylabel(\"Процент (%)\", fontsize=12)\n",
        "plt.xlabel(\"Оценка\", fontsize=12)\n",
        "plt.xticks(fontsize=11)\n",
        "plt.yticks(fontsize=11)\n",
        "\n",
        "# Убираем лишние границы\n",
        "sns.despine(left=True)\n",
        "\n",
        "# Добавляем горизонтальную сетку\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Оптимизация расположения элементов\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На этом графике видно распредление оценок в процентном соотношении и наблюдаеться что больше всего данных это 5 звезд"
      ],
      "metadata": {
        "id": "0J6p7iUpCGEO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA8foGvMsi9n"
      },
      "source": [
        "## Работа с текстом"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGH4iiCZMEXZ"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"blanchefort/rubert-base-cased-sentiment\"  # Поддерживает эмодзи и русский\n",
        "MAX_LENGTH = 256            # Увеличьте длину для контекста (если позволяет память)\n",
        "NUM_LABELS = 6\n",
        "SEED = 42\n",
        "TRAIN_BATCH_SIZE = 32      # Уменьшите batch для стабильности обучения\n",
        "EVAL_BATCH_SIZE = 32\n",
        "LEARNING_RATE = 3e-5        # Повышенный LR для tiny-модели\n",
        "WEIGHT_DECAY = 0.01\n",
        "NUM_EPOCHS = 35            # Уменьшите эпохи + ранняя остановка\n",
        "WARMUP_RATIO = 0.2          # Больше прогревов\n",
        "LOGGING_STEPS = 100\n",
        "SAVE_TOTAL_LIMIT = 2\n",
        "OUTPUT_DIR = \"./results\"\n",
        "LOGGING_DIR = \"./logs\"\n",
        "\n",
        "# Добавьте раннюю остановку\n",
        "from transformers import EarlyStoppingCallback\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Очистка текста"
      ],
      "metadata": {
        "id": "GbUCXt2PEmQy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jN6lagiSsVr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "\n",
        "# Очистка текста, не удаляя пунктуацию\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()  # Не удаляем пунктуацию, эмодзи и спецсимволы\n",
        "\n",
        "# Очистка комментариев\n",
        "df_clean['comment'] = df_clean['comment'].apply(clean_text)\n",
        "\n",
        "# Категории для классификации\n",
        "category_cols = [\n",
        "    'Вопрос решен',\n",
        "    'Нравится качество выполнения заявки',\n",
        "    'Нравится качество работы сотрудников',\n",
        "    'Нравится скорость отработки заявок',\n",
        "    'Понравилось выполнение заявки',\n",
        "    'Другое'\n",
        "]\n",
        "\n",
        "# Подготовка DataFrame\n",
        "df_model = df_clean[['comment'] + category_cols].copy()\n",
        "df_model = df_model.rename(columns={'comment': 'text'})\n",
        "\n",
        "# Приведение меток к int\n",
        "df_model[category_cols] = df_model[category_cols].astype(int)\n",
        "\n",
        "\n",
        "\n",
        "# Подсчёт весов для каждого класса: neg/pos (количество относящихся комментариев к классу и нет)\n",
        "all_labels = df_model[category_cols].values\n",
        "pos_counts = all_labels.sum(axis=0)  # Количество положительных примеров для каждого класса\n",
        "neg_counts = all_labels.shape[0] - pos_counts  # Количество отрицательных примеров для каждого класса\n",
        "pos_weight = torch.tensor(neg_counts / pos_counts, dtype=torch.float32)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Распределение классов:\")\n",
        "print(f\"Всего комментариев: {all_labels.shape[0]}\\n\")\n",
        "\n",
        "for i, col in enumerate(category_cols):\n",
        "    print(f\"Класс '{col}':\")\n",
        "    print(f\"  Положительных: {pos_counts[i]} ({pos_counts[i]/all_labels.shape[0]:.1%})\")\n",
        "    print(f\"  Отрицательных: {neg_counts[i]} ({neg_counts[i]/all_labels.shape[0]:.1%})\")\n",
        "    print(f\"  Вес класса (neg/pos): {pos_weight[i]:.2f}\\n\")\n",
        "\n",
        "print(\"\\nИтоговые веса для loss-функции:\")\n",
        "print(pos_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frjGUdivUZhc"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Создаём датасет HuggingFace\n",
        "dataset = Dataset.from_pandas(df_model)\n",
        "dataset = dataset.shuffle(seed=SEED)\n",
        "\n",
        "# Добавляем поле 'labels' как список значений категорий\n",
        "def format_labels(example):\n",
        "    example[\"labels\"] = [float(example[col]) for col in category_cols]\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(format_labels)\n",
        "\n",
        "# Удаляем отдельные метки (оставляем только 'text' и 'labels')\n",
        "dataset = dataset.remove_columns(category_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизация"
      ],
      "metadata": {
        "id": "ThH6LLlsE8vB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0rG-dFoUhDh"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "    )\n",
        "\n",
        "# Токенизируем\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка данных для обучения и стратифицированное разбиение"
      ],
      "metadata": {
        "id": "3NX35mgnF_Fa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-jUOaXNUg1w"
      },
      "outputs": [],
      "source": [
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from datasets import Dataset\n",
        "\n",
        "# === Стратифицированное разбиение по мультиразметке ===\n",
        "X = df_model['text'].values\n",
        "y = df_model[category_cols].values\n",
        "\n",
        "mskf = MultilabelStratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)\n",
        "splits = list(mskf.split(X, y))\n",
        "\n",
        "# Первый сплит: train+val и test\n",
        "train_val_idx, test_idx = splits[0]\n",
        "train_val_df = df_model.iloc[train_val_idx].reset_index(drop=True)\n",
        "test_df = df_model.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# Второй сплит внутри train_val: train и val\n",
        "inner_mskf = MultilabelStratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)\n",
        "inner_splits = list(inner_mskf.split(train_val_df['text'].values, train_val_df[category_cols].values))\n",
        "\n",
        "train_idx, val_idx = inner_splits[0]\n",
        "train_df = train_val_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_df = train_val_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "# === Преобразуем pandas -> Hugging Face Dataset ===\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# === Добавляем поле labels ===\n",
        "def format_labels(example):\n",
        "    example[\"labels\"] = [float(example[col]) for col in category_cols]\n",
        "    return example\n",
        "\n",
        "train_dataset = train_dataset.map(format_labels)\n",
        "val_dataset = val_dataset.map(format_labels)\n",
        "test_dataset = test_dataset.map(format_labels)\n",
        "\n",
        "# Удаляем категориальные метки (оставим только labels)\n",
        "train_dataset = train_dataset.remove_columns(category_cols)\n",
        "val_dataset = val_dataset.remove_columns(category_cols)\n",
        "test_dataset = test_dataset.remove_columns(category_cols)\n",
        "\n",
        "# === Токенизация ===\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Удалим лишние столбцы (оставим 'text' только в test_dataset)\n",
        "for ds_name, ds in zip(\n",
        "    [\"train\", \"val\", \"test\"], [train_dataset, val_dataset, test_dataset]\n",
        "):\n",
        "    if ds_name != \"test\" and \"text\" in ds.column_names:\n",
        "        ds = ds.remove_columns([\"text\"])\n",
        "    if \"_index_level_0_\" in ds.column_names:\n",
        "        ds = ds.remove_columns([\"_index_level_0_\"])\n",
        "    if ds_name == \"train\":\n",
        "        train_dataset = ds\n",
        "    elif ds_name == \"val\":\n",
        "        val_dataset = ds\n",
        "    else:\n",
        "        test_dataset = ds\n",
        "\n",
        "print(\"Порядок классов в labels:\", category_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Настройка модели и функции потерь"
      ],
      "metadata": {
        "id": "9VLeePNDHtAg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R2VGfsyaAxb"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class WeightedFocalLoss(torch.nn.Module):\n",
        "    def __init__(self, pos_weight=None, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer('pos_weight', pos_weight)\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Убедимся, что pos_weight на том же устройстве\n",
        "        pos_weight = self.pos_weight.to(inputs.device)\n",
        "\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(\n",
        "            inputs, targets, weight=pos_weight, reduction='none'\n",
        "        )\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * bce_loss\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeWSA4I6aAlQ"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, loss_fn=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # Удаляем num_items_in_batch, если он есть\n",
        "        inputs.pop('num_items_in_batch', None)\n",
        "\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        if self.loss_fn is not None:\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "        else:\n",
        "            loss = outputs.loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80irW2yiUgr6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_LABELS,\n",
        "    problem_type=\"multi_label_classification\",\n",
        "    # hidden_dropout_prob=0.3,       # Dropout между слоями\n",
        "    # attention_probs_dropout_prob=0.2  # Dropout в attention\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "loss_fn = WeightedFocalLoss(pos_weight=pos_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели и кросс-валидация"
      ],
      "metadata": {
        "id": "OHUhV5jYIL95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4G8vy7gVCSn"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score, roc_curve,\n",
        "    precision_score, recall_score, auc\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from datasets import Dataset\n",
        "from IPython.display import display\n",
        "\n",
        "# Отключаем wandb\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# === Метрики ===\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions\n",
        "    labels = p.label_ids\n",
        "    binary_preds = (preds > 0.5).astype(int)\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, preds, average='macro')\n",
        "    except ValueError:\n",
        "        roc_auc = float('nan')\n",
        "    acc = accuracy_score(labels, binary_preds)\n",
        "    f1 = f1_score(labels, binary_preds, average='macro')\n",
        "    return {\"accuracy\": acc, \"f1_macro\": f1, \"roc_auc_macro\": roc_auc}\n",
        "\n",
        "# === Аргументы обучения ===\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    logging_dir=LOGGING_DIR,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"roc_auc_macro\",\n",
        "    greater_is_better=True,\n",
        "    seed=SEED,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# === K-Fold обучение ===\n",
        "all_fold_metrics = []\n",
        "for fold, (train_idx, val_idx) in enumerate(mskf.split(X, y)):\n",
        "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
        "    train_df = df_model.iloc[train_idx].reset_index(drop=True)\n",
        "    val_df = df_model.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = Dataset.from_pandas(train_df).map(format_labels)\n",
        "    val_dataset = Dataset.from_pandas(val_df).map(format_labels)\n",
        "    train_dataset = train_dataset.remove_columns(category_cols).map(tokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.remove_columns(category_cols).map(tokenize_function, batched=True)\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=deepcopy(model),\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        loss_fn=loss_fn,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    val_logits = trainer.predict(val_dataset).predictions\n",
        "    val_probs = torch.sigmoid(torch.tensor(val_logits)).numpy()\n",
        "    binary_preds = (val_probs > 0.5).astype(int)\n",
        "\n",
        "    f1 = f1_score(val_df[category_cols].values, binary_preds, average='macro')\n",
        "    print(f\"Fold {fold + 1} F1: {f1:.4f}\")\n",
        "    all_fold_metrics.append(f1)\n",
        "\n",
        "print(f\"\\nСредний F1 по 5 фолдам: {np.mean(all_fold_metrics):.4f}\")\n",
        "\n",
        "# === Оптимизация порогов ===\n",
        "val_logits = trainer.predict(val_dataset).predictions\n",
        "val_probs = torch.sigmoid(torch.tensor(val_logits)).numpy()\n",
        "\n",
        "thresholds = []\n",
        "for i in range(len(category_cols)):\n",
        "    best_f1, best_t = 0, 0.5\n",
        "    for t in np.linspace(0.1, 0.9, 81):\n",
        "        preds = (val_probs[:, i] > t).astype(int)\n",
        "        f1 = f1_score(val_df[category_cols[i]].values, preds)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = f1, t\n",
        "    thresholds.append(best_t)\n",
        "\n",
        "print(\"Лучшие пороги для каждого класса:\", thresholds)\n",
        "\n",
        "# === Предсказания на тесте ===\n",
        "preds_output = trainer.predict(test_dataset)\n",
        "true_labels = preds_output.label_ids\n",
        "probs = torch.sigmoid(torch.tensor(preds_output.predictions)).numpy()\n",
        "\n",
        "final_preds = np.zeros_like(probs)\n",
        "for i, t in enumerate(thresholds):\n",
        "    final_preds[:, i] = (probs[:, i] > t).astype(int)\n",
        "\n",
        "# === Финальные метрики ===\n",
        "def compute_final_metrics(true_labels, final_preds, probs):\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(true_labels, final_preds),\n",
        "        \"f1_macro\": f1_score(true_labels, final_preds, average='macro'),\n",
        "        \"roc_auc_macro\": roc_auc_score(true_labels, probs, average='macro'),\n",
        "    }\n",
        "\n",
        "metrics = compute_final_metrics(true_labels, final_preds, probs)\n",
        "print(\"\\n=== Итоговые метрики по порогам ===\")\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# === Таблица предсказаний ===\n",
        "probs_df = pd.DataFrame(probs, columns=[col + '_prob' for col in category_cols])\n",
        "preds_df = pd.DataFrame(final_preds, columns=[col + '_pred' for col in category_cols])\n",
        "true_df = pd.DataFrame(true_labels, columns=[col + '_true' for col in category_cols])\n",
        "result_df = pd.concat([probs_df, preds_df, true_df], axis=1)\n",
        "\n",
        "print(\"\\n=== Таблица предсказаний (первые 10 строк) ===\")\n",
        "styled_table = (result_df.head(10)\n",
        "                .style.format(precision=2)\n",
        "                .set_properties({'text-align': 'center'})\n",
        "                .set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}]))\n",
        "display(styled_table)\n",
        "\n",
        "result_df.to_excel(\"test_predictions.xlsx\", index=False)\n",
        "\n",
        "# === ROC AUC кривые ===\n",
        "def compute_roc_auc(true_labels, predicted_probs, label_columns):\n",
        "    fpr, tpr, roc_auc = {}, {}, {}\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    line_styles = ['-', '-', '-', '-']\n",
        "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "    for i, label in enumerate(label_columns):\n",
        "        fpr[i], tpr[i], _ = roc_curve(true_labels[:, i], predicted_probs[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        plt.plot(fpr[i], tpr[i], linestyle=line_styles[i % len(line_styles)],\n",
        "                 color=colors[i % len(colors)], lw=2,\n",
        "                 label=f'{label} (AUC = {roc_auc[i]:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1, alpha=0.5)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    return roc_auc\n",
        "\n",
        "roc_auc = compute_roc_auc(true_labels, probs, category_cols)\n",
        "avg_roc_auc = np.mean(list(roc_auc.values()))\n",
        "print(f'\\nСредний ROC AUC: {avg_roc_auc:.2f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}